{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "# alternative way to create the spark context, if you have memory problems or timeout limits.\n",
    "#spark = SparkSession.builder \\\n",
    "#    .config(\"spark.executor.heartbeatInterval\", \"20000s\") \\\n",
    "#    .config(\"spark.network.timeout\", \"20001s\") \\\n",
    "#    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4\")\\\n",
    "#    .config(\"spark.driver.memory\",\"10G\")\\\n",
    "#    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "\n",
    "comet_ml.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.logging.comet import CometLogger\n",
    "logger = CometLogger()\n",
    "logger.experiment.set_name('PretrainedModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"source/DisneylandReviews.csv\")\n",
    "df = df.withColumn(\"sentiment\", when(col(\"Rating\") > 2, \"positive\").otherwise(\"negative\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.limit(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Pretrained pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "document = DocumentAssembler() \\\n",
    "    .setInputCol(\"Review_Text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "token = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normal\")\n",
    "\n",
    "vivekn =  ViveknSentimentModel.pretrained() \\\n",
    "    .setInputCols([\"document\", \"normal\"]) \\\n",
    "    .setOutputCol(\"result_sentiment\") \\\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"result_sentiment\"]) \\\n",
    "    .setOutputCols(\"final_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([document, token, normalizer, vivekn, finisher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.select('Review_Text').toDF('Review_Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(X)\n",
    "result = pipelineModel.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging the pipeline parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Evaluation in Comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df_tot = df.join(result, on=[\"Review_Text\"])\n",
    "pandas_df = df_tot.toPandas()\n",
    "pandas_df['predicted_sentiment'] = [','.join(map(str, l)) for l in pandas_df['final_sentiment']]\n",
    "\n",
    "report = classification_report(pandas_df['sentiment'], pandas_df['predicted_sentiment'], output_dict=True, labels=['positive', 'negative'])\n",
    "for key, value in report.items():\n",
    "    if key!='accuracy':\n",
    "        logger.log_metrics(value,prefix=key)\n",
    "    else:\n",
    "        logger.log_metrics({\"accuracy\": value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(pandas_df['sentiment'], pandas_df['predicted_sentiment'])\n",
    "logger.log_metrics({\"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CometLogger()\n",
    "logger.experiment.set_name('CustomModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vivekn_custom = ViveknSentimentApproach() \\\n",
    "    .setInputCols([\"document\", \"normal\"]) \\\n",
    "    .setSentimentCol(\"sentiment\") \\\n",
    "    .setOutputCol(\"result_sentiment\") \n",
    "    \n",
    "pipeline = Pipeline().setStages([document, token, normalizer, vivekn_custom, finisher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_set, test_set) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_set.select('Review_Text', 'sentiment').toDF('Review_Text', 'sentiment')\n",
    "X_test = test_set.select('Review_Text', 'sentiment').toDF('Review_Text', 'sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipelineModel.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Evaluation in Comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = result.select('sentiment', 'final_sentiment').toPandas()\n",
    "pandas_df['predicted_sentiment'] = [','.join(map(str, l)) for l in pandas_df['final_sentiment']]\n",
    "\n",
    "report = classification_report(pandas_df['sentiment'], pandas_df['predicted_sentiment'], output_dict=True, labels=['positive', 'negative'])\n",
    "for key, value in report.items():\n",
    "    if key!='accuracy':\n",
    "        logger.log_metrics(value,prefix=key)\n",
    "    else:\n",
    "        logger.log_metrics({\"accuracy\": value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(pandas_df['sentiment'], pandas_df['predicted_sentiment'])\n",
    "logger.log_metrics({\"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
